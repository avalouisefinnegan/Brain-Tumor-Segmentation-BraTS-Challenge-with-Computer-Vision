{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "# Pytorch functions\n",
    "import torch\n",
    "# Neural network layers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# Torchvision library\n",
    "from torchvision import transforms\n",
    "\n",
    "# For results\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_cuda = True, use_mps = False):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    if use_mps:\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "USE_SEED = True\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available(), torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Transformations\n",
    "\n",
    "These are some transformations I have written but need to be updated for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crop(object):\n",
    "    def __init__(self, output_ind):\n",
    "        self.output_ind = output_ind\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = []\n",
    "        output_ind = self.output_ind\n",
    "        for i in range(len(image)): # 4\n",
    "            new_image.append(image[i][output_ind[0][0]:output_ind[0][1], output_ind[1][0]:output_ind[1][1],:])\n",
    "            new_label = label[output_ind[0][0]:output_ind[0][1], output_ind[1][0]:output_ind[1][1],:]\n",
    "        return new_image, new_label\n",
    "\n",
    "class Flatten(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample # images have 4 image\n",
    "        new_image = []\n",
    "        for i in range(len(image)):\n",
    "            new_image.append(image[i].reshape(180, -1, order = 'F'))\n",
    "        new_label = label.reshape(-1)\n",
    "        return new_image, new_label\n",
    "    \n",
    "class ScanNormalize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = []\n",
    "        for i in range(len(image)):\n",
    "            img = image[i]\n",
    "            new_scan = (img-np.min(img))/(np.max(img)-np.min(img))\n",
    "            new_image.append(new_scan)\n",
    "        return new_image, label\n",
    "\n",
    "class StackScans(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = np.stack(image, axis=-1)\n",
    "        return new_image, label\n",
    "    \n",
    "class BinaryLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.sign(label)\n",
    "        return image, new_label\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image), torch.from_numpy(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D MRI Scans Inputs\n",
    "\n",
    "We load flair, t1ce, t2 and define any pre-processing methods here. Without preprocessing, the training data has size `(3, 240, 240, 155)` and the target has size `(240, 240, 155)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, image_path = r'./BraTS/BraTS2021_Training_Data', transform=None):\n",
    "        'Initialisation'\n",
    "        self.image_path = image_path\n",
    "        self.folders_name = [folder for folder in os.listdir(self.image_path) if folder != '.DS_Store']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.folders_name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Select sample\n",
    "        fld_name = self.folders_name[index]\n",
    "        image = []\n",
    "        for scan_type in ['flair', 't1ce', 't2']:\n",
    "            path_img = os.path.join(self.image_path, fld_name, fld_name + '_' + scan_type + '.nii.gz')\n",
    "            img = nib.load(path_img).get_fdata()\n",
    "            image.append(img)\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        path_label = os.path.join(self.image_path, fld_name, fld_name + '_seg.nii.gz')\n",
    "\n",
    "        label = nib.load(path_label).get_fdata()\n",
    "        if self.transform:\n",
    "            image, label = self.transform([image, label])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D MRI Scans Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, image_path = r'./BraTS/BraTS2021_Training_Data', transform=None):\n",
    "        'Initialisation'\n",
    "        self.image_path = image_path\n",
    "        self.folders_name = [folder for folder in os.listdir(self.image_path) if folder != '.DS_Store']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.folders_name) * 155\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Determine the image index and the RGB layer\n",
    "        image_idx = index // 155\n",
    "        layer_idx = index % 155\n",
    "\n",
    "        # Select sample\n",
    "        fld_name = self.folders_name[image_idx]\n",
    "        image = []\n",
    "        for scan_type in ['flair', 't1ce', 't2']:\n",
    "            path_img = os.path.join(self.image_path, fld_name, fld_name + '_' + scan_type + '.nii.gz')\n",
    "            img = nib.load(path_img).get_fdata()\n",
    "            # Need to apply standardisation here...\n",
    "            image.append(img[:, :, layer_idx])\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        path_label = os.path.join(self.image_path, fld_name, fld_name + '_seg.nii.gz')\n",
    "\n",
    "        label = nib.load(path_label).get_fdata()[:, :, layer_idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image, label = self.transform([image, label])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class everythirdlayer(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[:,:,:,np.arange(3, 152, 3)]\n",
    "        new_label = label[:,:,np.arange(3, 152, 3)]\n",
    "        return new_image, new_label\n",
    "\n",
    "class Flair(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[0]\n",
    "        return new_image, label\n",
    "    \n",
    "class IntLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = label.astype(int)\n",
    "        return image, new_label\n",
    "    \n",
    "class BinariseLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.sign(label)\n",
    "        return image, new_label\n",
    "\n",
    "class CropAndResize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        rows = np.any(image, axis=1)\n",
    "        cols = np.any(image, axis=0)\n",
    "\n",
    "        # Find the bounding box of the non-zero regions\n",
    "        rows_indices = np.where(rows)[0]\n",
    "        cols_indices = np.where(cols)[0]\n",
    "        top_row = np.min(rows_indices)\n",
    "        bottom_row = np.max(rows_indices)\n",
    "        left_col = np.min(cols_indices)\n",
    "        right_col = np.max(cols_indices)\n",
    "\n",
    "        square_size = max(bottom_row - top_row, right_col - left_col) + 1\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_image = image[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "        cropped_label = label[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "\n",
    "        # Resize the image\n",
    "        dim = [64,64]\n",
    "        resized_image = cv2.resize(cropped_image, dim)\n",
    "        resized_label = cv2.resize(cropped_label, dim)\n",
    "\n",
    "        return resized_image, resized_label\n",
    "\n",
    "class Standardise(object) :\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        image_no_zeros = np.where(image == 0, np.nan, image)\n",
    "        new_image = (image_no_zeros - np.nanmean(image_no_zeros)) / np.nanstd(image_no_zeros)\n",
    "        new_image = np.nan_to_num(new_image)\n",
    "        return new_image, label\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        # image = image.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDataset(image_path = r'./BraTS/BraTS2021_Training_Data',\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Flair(),\n",
    "                                        CropAndResize(),\n",
    "                                        Standardise(),\n",
    "                                        IntLabel(),\n",
    "                                        BinariseLabel(),\n",
    "                                        ToTensor()\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/\n",
    "https://github.com/facebookresearch/detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_val_test_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "dataset_size = int(len(dataset)/155)\n",
    "dataset_indices = list(range(dataset_size))\n",
    "\n",
    "train_sampler, val_sampler, test_sampler = random_split(dataset_indices, train_val_test_split, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler.indices = [i * 155 + j for i in train_sampler.indices for j in range(155)]\n",
    "val_sampler.indices = [i * 155 + j for i in val_sampler.indices for j in range(155)]\n",
    "test_sampler.indices = [i * 155 + j for i in test_sampler.indices for j in range(155)]\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(train_sampler.indices)\n",
    "random.shuffle(val_sampler.indices)\n",
    "random.shuffle(test_sampler.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(dataset, batch_size=batch_size,\n",
    "                            sampler=train_sampler)\n",
    "validation_iterator = DataLoader(dataset, batch_size=batch_size,\n",
    "                            sampler=val_sampler)\n",
    "test_iterator = DataLoader(dataset, batch_size=batch_size, sampler = test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.features = nn.Sequential(\n",
    "      ## encoder layers ##\n",
    "      # conv layer (depth from 1 --> 4), 3x3 kernels\n",
    "      nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3),\n",
    "      nn.ReLU(),\n",
    "      # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "      nn.MaxPool2d(2),\n",
    "      # conv layer (depth from 4 --> 8), 4x4 kernels\n",
    "      nn.Conv2d(in_channels=4, out_channels=8, kernel_size=4),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2),\n",
    "      # conv layer (depth from 8 --> 12), 5x5 kernels\n",
    "      nn.Conv2d(in_channels=8, out_channels=12, kernel_size=5),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(3),\n",
    "      ## decoder layers ##\n",
    "      # add transpose conv layers, with relu activation function\n",
    "      ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "      nn.ConvTranspose2d(12, 8, 2, stride=3),\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(8, 4, 2, stride=2),\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(4, 1, 2, stride=2),\n",
    "      # output layer (with sigmoid for scaling from 0 to 1)\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    # self.linear = nn.Sequential(\n",
    "    #   nn.Linear(256 * 5 * 5 * 5, 383625),\n",
    "    #   nn.ReLU(),\n",
    "    #   nn.Linear(383625, 1534500),\n",
    "    #   nn.ReLU(),\n",
    "    #   nn.Linear(1534500, output_dim)\n",
    "    # )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.features(x)\n",
    "    # x = x.view(x.shape[0], -1)\n",
    "    # x = self.linear(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,513 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "# criterion = nn.CrossEntropyLoss() # Softmax + CrossEntropy\n",
    "criterion = nn.BCELoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/30 -- Epoch Time: 0.01 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 2/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 3/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 4/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 5/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 6/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 7/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 8/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 9/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 10/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 11/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 12/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 13/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 14/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 15/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 16/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 17/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 18/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 19/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 20/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 21/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 22/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 23/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 24/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 25/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 26/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 27/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 28/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 29/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n",
      "\n",
      "Epoch: 30/30 -- Epoch Time: 0.00 s\n",
      "---------------------------------\n",
      "Train -- Loss: 0.000, Acc: 0.00%\n",
      "Val -- Loss: 0.000, Acc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 30\n",
    "train_losses, train_accs, valid_losses, valid_accs = model_training(N_EPOCHS,\n",
    "                                                                    model,\n",
    "                                                                    train_iterator,\n",
    "                                                                    validation_iterator,\n",
    "                                                                    optimizer,\n",
    "                                                                    criterion,\n",
    "                                                                    device,\n",
    "                                                                    'CNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing(model, test_iterator, criterion, device, 'CNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_report(model, test_iterator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
