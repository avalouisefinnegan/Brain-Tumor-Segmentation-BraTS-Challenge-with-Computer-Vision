{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utilities import *\n",
    "\n",
    "# Pytorch functions\n",
    "import torch\n",
    "# Neural network layers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "# Torchvision library\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# For results\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed, use_cuda = True, use_mps = False):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    if use_mps:\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "SEED = 44\n",
    "\n",
    "USE_SEED = True\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available(), torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Slice Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, image_path = r'./BraTS/BraTS2021_Training_Data', transform=None):\n",
    "        'Initialisation'\n",
    "        self.image_path = image_path\n",
    "        self.folders_name = [folder for folder in os.listdir(self.image_path) if folder != '.DS_Store']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.folders_name) * 155\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Determine the image index and the RGB layer\n",
    "        image_idx = index // 155\n",
    "        layer_idx = index % 155\n",
    "\n",
    "        # Select sample\n",
    "        fld_name = self.folders_name[image_idx]\n",
    "        image = []\n",
    "        for scan_type in ['flair', 't1ce', 't2', 't1']:\n",
    "            path_img = os.path.join(self.image_path, fld_name, fld_name + '_' + scan_type + '.nii.gz')\n",
    "            img = nib.load(path_img).get_fdata()\n",
    "            # Need to apply standardisation here...\n",
    "            image.append(img[:, :, layer_idx])\n",
    "        \n",
    "        image = np.array(image)\n",
    "\n",
    "        path_label = os.path.join(self.image_path, fld_name, fld_name + '_seg.nii.gz')\n",
    "\n",
    "        label = nib.load(path_label).get_fdata()[:, :, layer_idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image, label = self.transform([image, label])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class everythirdlayer(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[:,:,:,np.arange(3, 152, 3)]\n",
    "        new_label = label[:,:,np.arange(3, 152, 3)]\n",
    "        return new_image, new_label\n",
    "\n",
    "class Flair(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[0]\n",
    "        return new_image, label\n",
    "    \n",
    "class T1CE(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[1]\n",
    "        return new_image, label\n",
    "\n",
    "class T2(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[2]\n",
    "        return new_image, label\n",
    "\n",
    "class T1(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[3]\n",
    "        return new_image, label\n",
    "    \n",
    "class IntLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = label.astype(int)\n",
    "        return image, new_label\n",
    "    \n",
    "class BinariseLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.sign(label)\n",
    "        return image, new_label\n",
    "\n",
    "class CropAndResize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        rows = np.any(image, axis=1)\n",
    "        cols = np.any(image, axis=0)\n",
    "\n",
    "        # Find the bounding box of the non-zero regions\n",
    "        rows_indices = np.where(rows)[0]\n",
    "        cols_indices = np.where(cols)[0]\n",
    "        if len(rows_indices) != 0 or len(cols_indices) != 0:\n",
    "            top_row = np.min(rows_indices)\n",
    "            bottom_row = np.max(rows_indices)\n",
    "            left_col = np.min(cols_indices)\n",
    "            right_col = np.max(cols_indices)\n",
    "\n",
    "            square_size = max(bottom_row - top_row, right_col - left_col) + 1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "            cropped_label = label[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "        else:\n",
    "            cropped_image = image\n",
    "            cropped_label = label\n",
    "        # Resize the image\n",
    "        dim = [64,64]\n",
    "        resized_image = cv2.resize(cropped_image, dim)\n",
    "        resized_label = cv2.resize(cropped_label, dim)\n",
    "\n",
    "        return resized_image, resized_label\n",
    "\n",
    "class Standardise(object) :\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        if np.abs(image).sum() == 0:\n",
    "            return image, label\n",
    "        with np.errstate(divide='ignore',invalid='ignore'):\n",
    "            image_no_zeros = np.where(image == 0, np.nan, image)\n",
    "            new_image = (image_no_zeros - np.nanmean(image_no_zeros)) / np.nanstd(image_no_zeros)\n",
    "            new_image = np.nan_to_num(new_image)\n",
    "        return new_image, label\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        image = np.float32(image)\n",
    "        label = np.float32(label)\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        # image = image.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDataset(image_path = r'./BraTS/BraTS2021_Training_Data',\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Flair(),\n",
    "                                        CropAndResize(),\n",
    "                                        Standardise(),\n",
    "                                        IntLabel(),\n",
    "                                        BinariseLabel(),\n",
    "                                        ToTensor()\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(4,4, figsize=(12,12))\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(dataset[i * 10][0])\n",
    "plt.show()\n",
    "fig, axes = plt.subplots(4,4, figsize=(12,12))\n",
    "for i, ax in enumerate(axes.reshape(-1)):\n",
    "    ax.imshow(dataset[i * 10][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_test_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "# generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "dataset_size = int(len(dataset)/155)\n",
    "dataset_indices = list(range(dataset_size))\n",
    "\n",
    "train_indices, test_indices = train_test_split(dataset_indices, test_size=0.1, random_state=SEED)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.22, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = [[],[],[]]\n",
    "for i, ind_list in enumerate([train_indices, val_indices, test_indices]):\n",
    "    for ind in ind_list:\n",
    "        for j in range(155):\n",
    "            tmp_list[i].append(ind*155 + j)\n",
    "train_indices, val_indices, test_indices = tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create the subset DataLoader\n",
    "train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# multiprocessing_context=\"forkserver\"\n",
    "# persistent_workers=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Docs\n",
    "\n",
    "[nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
    "\n",
    "[nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "\n",
    "[nn.ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2d_output(i, p, d, k, s):\n",
    "    return (i + 2*p - d*(k-1) - 1)/s + 1\n",
    "def ConvTranspose2d_output(i, p, d, k, s): # p is output_padding\n",
    "    return (i-1)*s+d*(k-1)+p+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.features = nn.Sequential(\n",
    "      ## encoder layers ##\n",
    "      # conv layer (depth from 1 --> 4), 3x3 kernels\n",
    "      # Input 64 x 64\n",
    "      nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding = 'same'), # 64 x 64\n",
    "      nn.ReLU(),\n",
    "      # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "      nn.MaxPool2d(2), ## 32 x 32\n",
    "      # conv layer (depth from 4 --> 8), 4x4 kernels\n",
    "      nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding = 'same'), # 32 x 32\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2), # 16 x 16\n",
    "      # conv layer (depth from 8 --> 12), 5x5 kernels\n",
    "      nn.Conv2d(in_channels=8, out_channels=12, kernel_size=3, padding = 'same'), # ( 12 x ) 16 x 16\n",
    "      nn.ReLU(),\n",
    "      \n",
    "      ## decoder layers ##\n",
    "      # add transpose conv layers, with relu activation function\n",
    "      nn.ConvTranspose2d(12, 6, kernel_size = 2, stride=2), # 32 x 32\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(6, 1, kernel_size = 2, stride=2), # 64 x 64\n",
    "      # output layer (with sigmoid for scaling from 0 to 1)\n",
    "      # nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.view(int(np.prod(x.shape)/(64**2)), 1, 64, 64)\n",
    "    x = self.features(x)\n",
    "    # x = x.view(x.shape[0], -1)\n",
    "    # x = torch.flatten(x, start_dim=1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,531 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "# criterion = nn.CrossEntropyLoss() # Softmax + CrossEntropy\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, valid_losses, valid_accs = model_training(N_EPOCHS,\n",
    "                                                                    model,\n",
    "                                                                    train_dataloader,\n",
    "                                                                    val_dataloader,\n",
    "                                                                    optimizer,\n",
    "                                                                    criterion,\n",
    "                                                                    device,\n",
    "                                                                    'CNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_testing(model, test_dataloader, criterion, device, True, 'CNN_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, pred = predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "model.load_state_dict(torch.load('CNN.pt'))\n",
    "evaluate(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('CNN.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, valid_losses, valid_accs = model_training(N_EPOCHS,\n",
    "                                                                    model,\n",
    "                                                                    train_dataloader,\n",
    "                                                                    val_dataloader,\n",
    "                                                                    optimizer,\n",
    "                                                                    criterion,\n",
    "                                                                    device,\n",
    "                                                                    'CNN_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('CNN_2.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, valid_losses, valid_accs = model_training(N_EPOCHS,\n",
    "                                                                    model,\n",
    "                                                                    train_dataloader,\n",
    "                                                                    val_dataloader,\n",
    "                                                                    optimizer,\n",
    "                                                                    criterion,\n",
    "                                                                    device,\n",
    "                                                                    'CNN_3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "\n",
    "Training\n",
    "- Crop each image by the size of the tumor\n",
    "- Resize them (idealy small)\n",
    "- One hot encoding for each voxel\n",
    "\n",
    "Model input size\n",
    "$$(16 \\times 16)\\times 4\\text{ levels of classifications}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flair(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[0]\n",
    "        return new_image, label\n",
    "\n",
    "class T1CE(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[1]\n",
    "        return new_image, label\n",
    "\n",
    "class T2(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[2]\n",
    "        return new_image, label\n",
    "\n",
    "class T1(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[3]\n",
    "        return new_image, label\n",
    "    \n",
    "class IntLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = label.astype(int)\n",
    "        return image, new_label\n",
    "\n",
    "class CropAndResize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        rows = np.any(label, axis=1)\n",
    "        cols = np.any(label, axis=0)\n",
    "\n",
    "        # Find the bounding box of the non-zero regions\n",
    "        rows_indices = np.where(rows)[0]\n",
    "        cols_indices = np.where(cols)[0]\n",
    "        if len(rows_indices) != 0 or len(cols_indices) != 0:\n",
    "            top_row = np.min(rows_indices)\n",
    "            bottom_row = np.max(rows_indices)\n",
    "            left_col = np.min(cols_indices)\n",
    "            right_col = np.max(cols_indices)\n",
    "\n",
    "            square_size = max(bottom_row - top_row, right_col - left_col) + 1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "            cropped_label = label[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "        else:\n",
    "            cropped_image = image\n",
    "            cropped_label = label\n",
    "        # Resize the image\n",
    "        dim = [32,32]\n",
    "        resized_image = cv2.resize(cropped_image, dim)\n",
    "        resized_label = cv2.resize(cropped_label, dim)\n",
    "\n",
    "        return resized_image, resized_label\n",
    "\n",
    "class Standardise(object) :\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        if np.abs(image).sum() == 0:\n",
    "            return image, label\n",
    "        with np.errstate(divide='ignore',invalid='ignore'):\n",
    "            image_no_zeros = np.where(image == 0, np.nan, image)\n",
    "            new_image = (image_no_zeros - np.nanmean(image_no_zeros)) / np.nanstd(image_no_zeros)\n",
    "            new_image = np.nan_to_num(new_image)\n",
    "        return new_image, label\n",
    "    \n",
    "class OneHotLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.zeros((32,32,4)) # 0,1,2,4\n",
    "        for i, l in enumerate([0,1,2,4]):\n",
    "            new_label[:,:,i] = (label == l)\n",
    "        return image, new_label\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        image = np.float32(image)\n",
    "        label = np.float32(label)\n",
    "\n",
    "        # swap channel axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        new_label = label.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image), torch.from_numpy(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDataset(image_path = r'./BraTS/BraTS2021_Training_Data',\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Flair(),\n",
    "                                        CropAndResize(),\n",
    "                                        Standardise(),\n",
    "                                        IntLabel(),\n",
    "                                        OneHotLabel(),\n",
    "                                        ToTensor()\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create the subset DataLoader\n",
    "train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Input is 32 x 32 x 1\n",
    "        ## Output is 32 x 32 x 4\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3 32 x 32 x 1\n",
    "        self.e11 = nn.Conv2d(1, 64, kernel_size=3, padding=1) # output: 30x30x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, 32, 32)\n",
    "        # Encoder\n",
    "        xe11 = F.relu(self.e11(x))\n",
    "        xe12 = F.relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = F.relu(self.e21(xp1))\n",
    "        xe22 = F.relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = F.relu(self.e31(xp2))\n",
    "        xe32 = F.relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = F.relu(self.e41(xp3))\n",
    "        xe42 = F.relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = F.relu(self.e51(xp4))\n",
    "        xe52 = F.relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = F.relu(self.d11(xu11))\n",
    "        xd12 = F.relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = F.relu(self.d21(xu22))\n",
    "        xd22 = F.relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = F.relu(self.d31(xu33))\n",
    "        xd32 = F.relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = F.relu(self.d41(xu44))\n",
    "        xd42 = F.relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]             640\n",
      "            Conv2d-2           [-1, 64, 32, 32]          36,928\n",
      "         MaxPool2d-3           [-1, 64, 16, 16]               0\n",
      "            Conv2d-4          [-1, 128, 16, 16]          73,856\n",
      "            Conv2d-5          [-1, 128, 16, 16]         147,584\n",
      "         MaxPool2d-6            [-1, 128, 8, 8]               0\n",
      "            Conv2d-7            [-1, 256, 8, 8]         295,168\n",
      "            Conv2d-8            [-1, 256, 8, 8]         590,080\n",
      "         MaxPool2d-9            [-1, 256, 4, 4]               0\n",
      "           Conv2d-10            [-1, 512, 4, 4]       1,180,160\n",
      "           Conv2d-11            [-1, 512, 4, 4]       2,359,808\n",
      "        MaxPool2d-12            [-1, 512, 2, 2]               0\n",
      "           Conv2d-13           [-1, 1024, 2, 2]       4,719,616\n",
      "           Conv2d-14           [-1, 1024, 2, 2]       9,438,208\n",
      "  ConvTranspose2d-15            [-1, 512, 4, 4]       2,097,664\n",
      "           Conv2d-16            [-1, 512, 4, 4]       4,719,104\n",
      "           Conv2d-17            [-1, 512, 4, 4]       2,359,808\n",
      "  ConvTranspose2d-18            [-1, 256, 8, 8]         524,544\n",
      "           Conv2d-19            [-1, 256, 8, 8]       1,179,904\n",
      "           Conv2d-20            [-1, 256, 8, 8]         590,080\n",
      "  ConvTranspose2d-21          [-1, 128, 16, 16]         131,200\n",
      "           Conv2d-22          [-1, 128, 16, 16]         295,040\n",
      "           Conv2d-23          [-1, 128, 16, 16]         147,584\n",
      "  ConvTranspose2d-24           [-1, 64, 32, 32]          32,832\n",
      "           Conv2d-25           [-1, 64, 32, 32]          73,792\n",
      "           Conv2d-26           [-1, 64, 32, 32]          36,928\n",
      "           Conv2d-27            [-1, 4, 32, 32]             260\n",
      "================================================================\n",
      "Total params: 31,030,788\n",
      "Trainable params: 31,030,788\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 5.02\n",
      "Params size (MB): 118.37\n",
      "Estimated Total Size (MB): 123.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = UNet().to('cpu')\n",
    "summary(model, (32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()\n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "\n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "\n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "# criterion = nn.CrossEntropyLoss() # Softmax + CrossEntropy\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, valid_losses, valid_accs = model_training(N_EPOCHS,\n",
    "                                                                    model,\n",
    "                                                                    train_dataloader,\n",
    "                                                                    val_dataloader,\n",
    "                                                                    optimizer,\n",
    "                                                                    criterion,\n",
    "                                                                    device,\n",
    "                                                                    'UNet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 3\n",
    "model.load_state_dict(torch.load('UNet.pt'))\n",
    "model.to(device)\n",
    "train_losses, train_accs, valid_losses, valid_accs = model_training(N_EPOCHS,\n",
    "                                                                    model,\n",
    "                                                                    train_dataloader,\n",
    "                                                                    val_dataloader,\n",
    "                                                                    optimizer,\n",
    "                                                                    criterion,\n",
    "                                                                    device,\n",
    "                                                                    'UNet2.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict entire pipeline\n",
    "\n",
    "## Preprocessing for individual models\n",
    "\n",
    "First model\n",
    "- `Flair()`,\n",
    "- `CropAndResize()`,\n",
    "- `Standardise()`,\n",
    "- `IntLabel()`,\n",
    "- `BinariseLabel()`,\n",
    "- `ToTensor()`\n",
    "\n",
    "Second model\n",
    "- `Flair()`,\n",
    "- `CropAndResize()`,\n",
    "- `Standardise()`,\n",
    "- `IntLabel()`,\n",
    "- `OneHotLabel()`,\n",
    "- `ToTensor()`\n",
    "\n",
    "## Preprocessing for combined model\n",
    "\n",
    "- `Flair()`,\n",
    "- `CropAndResize1()`,\n",
    "- `Standardise()`,\n",
    "- `IntLabel()`,\n",
    "- `OneHotLabel()`,\n",
    "- `ToTensor()`\n",
    "\n",
    "First model\n",
    "- Model output: region of tumour\n",
    "\n",
    "Second model\n",
    "- Input: Crop the image `x` and resize based on the region of tumor `pred1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class everythirdlayer(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[:,:,:,np.arange(3, 152, 3)]\n",
    "        new_label = label[:,:,np.arange(3, 152, 3)]\n",
    "        return new_image, new_label\n",
    "\n",
    "class Flair(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_image = image[0]\n",
    "        return new_image, label\n",
    "    \n",
    "class IntLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = label.astype(int)\n",
    "        return image, new_label\n",
    "    \n",
    "class BinariseLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.sign(label)\n",
    "        return image, new_label\n",
    "\n",
    "class CropAndResize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        rows = np.any(image, axis=1)\n",
    "        cols = np.any(image, axis=0)\n",
    "\n",
    "        # Find the bounding box of the non-zero regions\n",
    "        rows_indices = np.where(rows)[0]\n",
    "        cols_indices = np.where(cols)[0]\n",
    "        if len(rows_indices) != 0 or len(cols_indices) != 0:\n",
    "            top_row = np.min(rows_indices)\n",
    "            bottom_row = np.max(rows_indices)\n",
    "            left_col = np.min(cols_indices)\n",
    "            right_col = np.max(cols_indices)\n",
    "\n",
    "            square_size = max(bottom_row - top_row, right_col - left_col) + 1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = image[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "            cropped_label = label[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "        else:\n",
    "            cropped_image = image\n",
    "            cropped_label = label\n",
    "        # Resize the image\n",
    "        dim = [64,64]\n",
    "        resized_image = cv2.resize(cropped_image, dim)\n",
    "        resized_label = cv2.resize(cropped_label, dim)\n",
    "\n",
    "        return resized_image, resized_label\n",
    "\n",
    "class Standardise(object) :\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        if np.abs(image).sum() == 0:\n",
    "            return image, label\n",
    "        with np.errstate(divide='ignore',invalid='ignore'):\n",
    "            image_no_zeros = np.where(image == 0, np.nan, image)\n",
    "            new_image = (image_no_zeros - np.nanmean(image_no_zeros)) / np.nanstd(image_no_zeros)\n",
    "            new_image = np.nan_to_num(new_image)\n",
    "        return new_image, label\n",
    "\n",
    "class OneHotLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.zeros((64,64,4)) # 0,1,2,4\n",
    "        for i, l in enumerate([0,1,2,4]):\n",
    "            new_label[:,:,i] = (label == l)\n",
    "        return image, new_label\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        image = np.float32(image)\n",
    "        label = np.float32(label)\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        # image = image.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bridge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize any parameters or setup needed for your custom function\n",
    "\n",
    "    def forward(self, x, pred1):\n",
    "        x = x.view(int(np.prod(x.shape)/(64**2)), 1, 64, 64)\n",
    "        thresholded_pred1 = torch.where(pred1 < 0, torch.tensor(0).to(device), torch.tensor(1).to(device))\n",
    "        rows = torch.any(thresholded_pred1, dim=2)\n",
    "        cols = torch.any(thresholded_pred1, dim=3)\n",
    "        rows_indices = torch.where(rows)[0]\n",
    "        cols_indices = torch.where(cols)[0]\n",
    "        if len(rows_indices) != 0 or len(cols_indices) != 0:\n",
    "            top_row = torch.min(rows_indices)\n",
    "            bottom_row = torch.max(rows_indices)\n",
    "            left_col = torch.min(cols_indices)\n",
    "            right_col = torch.max(cols_indices)\n",
    "\n",
    "            square_size = max(bottom_row - top_row, right_col - left_col) + 1\n",
    "\n",
    "            # Crop the image\n",
    "            cropped_image = x[top_row:top_row + square_size, left_col:left_col + square_size]\n",
    "        else:\n",
    "            cropped_image = x\n",
    "        resized_image = transforms.Resize(32)(cropped_image)\n",
    "\n",
    "        return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super().__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.bridge = Bridge()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred1 = self.model1(x) # x and pred_1 have size 64*64\n",
    "        pred2 = self.bridge(x, pred1) # Crop x according to pred1\n",
    "        pred3 = self.model2(pred2) # pred_2 has size 32*32\n",
    "        return pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models and load state_dicts    \n",
    "model1 = CNN()\n",
    "model2 = UNet()\n",
    "# Load state dicts\n",
    "model1.load_state_dict(torch.load('CNN_3.pt'))\n",
    "model2.load_state_dict(torch.load('UNet.pt'))\n",
    "\n",
    "model = pipeline(model1, model2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BraTSDataset(image_path = r'./BraTS/BraTS2021_Training_Data',\n",
    "                                    transform=transforms.Compose([\n",
    "                                        Flair(),\n",
    "                                        CropAndResize(),\n",
    "                                        Standardise(),\n",
    "                                        IntLabel(),\n",
    "                                        OneHotLabel(),\n",
    "                                        ToTensor()\n",
    "                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val_test_split = [0.7, 0.2, 0.1]\n",
    "\n",
    "# generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "dataset_size = int(len(dataset)/155)\n",
    "dataset_indices = list(range(dataset_size))\n",
    "\n",
    "train_indices, test_indices = train_test_split(dataset_indices, test_size=0.1, random_state=SEED)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.22, random_state=SEED)\n",
    "\n",
    "tmp_list = [[],[],[]]\n",
    "for i, ind_list in enumerate([train_indices, val_indices, test_indices]):\n",
    "    for ind in ind_list:\n",
    "        for j in range(155):\n",
    "            tmp_list[i].append(ind*155 + j)\n",
    "train_indices, val_indices, test_indices = tmp_list\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create the subset DataLoader\n",
    "train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, label = predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19530, 64, 64, 4]) torch.Size([19530, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([256, 64, 64, 4])) must be the same as input size (torch.Size([256, 4, 32, 32]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Brain-Tumor-Segmentation-BraTS-Challenge-with-Computer-Vision/utilities.py:120\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion, device, save)\u001b[0m\n\u001b[1;32m    117\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# [256, 64, 64]\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Compute accuracy\u001b[39;00m\n\u001b[1;32m    123\u001b[0m acc \u001b[38;5;241m=\u001b[39m calculate_accuracy(y_pred, y)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/modules/loss.py:725\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/torch/nn/functional.py:3197\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3194\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([256, 64, 64, 4])) must be the same as input size (torch.Size([256, 4, 32, 32]))"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict one sample\n",
    "\n",
    "- Pick a random sample from test data\n",
    "- `labels, pred = predict(model, iterator, device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('CNN_3.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample from the test dataset\n",
    "random.seed(SEED)\n",
    "random_sample = random.choice(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_one(model, sample_id, dataset, device):\n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    labels = []\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x, y = dataset[sample_id]\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze(1)\n",
    "\n",
    "        ## final prediction with a cut off probability\n",
    "        y_pred = (y_pred>0).float()\n",
    "\n",
    "        labels.append(y.cpu())\n",
    "        pred.append(y_pred.cpu())\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    pred = torch.cat(pred, dim=0)\n",
    "\n",
    "    return labels, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1, pred1 = pred_one(model, random_sample, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred1[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have `pred1`, how to fit this into the second model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
