{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Dataset Analysis\n",
    "\n",
    "This notebook contains the prediction of the test data we reserved for in `1_train.ipynb`. We will compute the full pipeline model's accuracy based on DICE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time, multiprocessing, glob, cv2, numpy as np, pandas as pd, nibabel as nib, matplotlib.pylab as plt\n",
    "\n",
    "# Pytorch functions\n",
    "import torch\n",
    "# Neural network layers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Optimizer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "# Torchvision library\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# For results\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utilities import *\n",
    "from preprocessing_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code block, we are running the PyTorch configuration, dataloaders, model architecture as we had defined during trainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 40\n",
    "batch_size = 64\n",
    "scan_type = 'Flair'\n",
    "master_path = r'./BraTS/'\n",
    "\n",
    "SEED = 44\n",
    "USE_SEED = True\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "def set_seed(seed, use_cuda = True, use_mps = False):\n",
    "    \"\"\"\n",
    "    Set SEED for PyTorch reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    if use_mps:\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "if USE_SEED:\n",
    "    set_seed(SEED, torch.cuda.is_available(), torch.backends.mps.is_available())\n",
    "\n",
    "## Define Custom Dataset\n",
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, image_path = r'./BraTS/BraTS2021_Training_Data_Slice', transform=None):\n",
    "        'Initialisation'\n",
    "        self.image_path = image_path\n",
    "        self.folders_name = [folder for folder in os.listdir(self.image_path) if folder != '.DS_Store']\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.folders_name) * 155\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Determine the image index and the RGB layer\n",
    "        image_idx = index // 155\n",
    "        layer_idx = index % 155\n",
    "\n",
    "        # Select sample\n",
    "        file_name = self.folders_name[image_idx]\n",
    "        \n",
    "        path_img = os.path.join(self.image_path, file_name, scan_type.lower(), file_name + '_' + scan_type.lower() + '_' + str(layer_idx+1) + '.npy')\n",
    "        image = np.load(path_img).astype(np.float32)\n",
    "\n",
    "        path_label = os.path.join(self.image_path, file_name, 'seg', file_name + '_seg_' + str(layer_idx+1) + '.npy')\n",
    "        label = np.load(path_label)\n",
    "        \n",
    "        if self.transform:\n",
    "            image, label = self.transform([image, label])\n",
    "        return image, label\n",
    "    \n",
    "class BinariseLabel(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        new_label = np.sign(label)\n",
    "        return image, new_label\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "    \n",
    "dataset = BraTSDataset(image_path = r'./BraTS/BraTS2021_Training_Data_Slice',\n",
    "                        transform=transforms.Compose([\n",
    "                            BinariseLabel(),\n",
    "                            ToTensor()\n",
    "                        ]))\n",
    "## Train Test Split\n",
    "dataset_size = int(len(dataset)/155)\n",
    "dataset_indices = list(range(dataset_size))\n",
    "\n",
    "train_indices, test_indices = train_test_split(dataset_indices, test_size=0.1, random_state=SEED)\n",
    "# train_indices, val_indices = train_test_split(train_indices, test_size=0.22, random_state=SEED)\n",
    "\n",
    "test_indices = []\n",
    "for ind in test_indices:\n",
    "    for j in range(155):\n",
    "        test_indices.append(ind*155 + j)\n",
    "\n",
    "# tmp_list = [[],[],[]]\n",
    "# for i, ind_list in enumerate([train_indices, val_indices, test_indices]):\n",
    "#     for ind in ind_list:\n",
    "#         for j in range(155):\n",
    "#             tmp_list[i].append(ind*155 + j)\n",
    "# train_indices, val_indices, test_indices = tmp_list\n",
    "\n",
    "# train_subset = Subset(dataset, train_indices)\n",
    "# val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create the subset DataLoader\n",
    "# train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Define Convlutional Autoencoder Structure\n",
    "class ConvAutoencoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.features = nn.Sequential(\n",
    "      ## encoder layers ##\n",
    "      # conv layer (depth from 1 --> 4), 3x3 kernels\n",
    "      # Input 64 x 64\n",
    "      nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding = 'same'), # 64 x 64\n",
    "      nn.ReLU(),\n",
    "      # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "      nn.MaxPool2d(2), ## 32 x 32\n",
    "      # conv layer (depth from 4 --> 8), 4x4 kernels\n",
    "      nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding = 'same'), # 32 x 32\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2), # 16 x 16\n",
    "      # conv layer (depth from 8 --> 12), 5x5 kernels\n",
    "      nn.Conv2d(in_channels=8, out_channels=12, kernel_size=3, padding = 'same'), # ( 12 x ) 16 x 16\n",
    "      nn.ReLU(),\n",
    "      \n",
    "      ## decoder layers ##\n",
    "      # add transpose conv layers, with relu activation function\n",
    "      nn.ConvTranspose2d(12, 6, kernel_size = 2, stride=2), # 32 x 32\n",
    "      nn.ReLU(),\n",
    "      nn.ConvTranspose2d(6, 1, kernel_size = 2, stride=2), # 64 x 64\n",
    "      # output layer (with sigmoid for scaling from 0 to 1)\n",
    "      # nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.view(int(np.prod(x.shape)/(64**2)), 1, 64, 64)\n",
    "    x = self.features(x)\n",
    "    return x\n",
    "model = ConvAutoencoder().to(device)\n",
    "\n",
    "## Loss function and Optimisation Methods\n",
    "# Loss\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "\n",
    "# Load the trained parameters\n",
    "model.load_state_dict(torch.load(f'./models/CA_{scan_type}.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the prediction from the Convolutional Autoencoder, the results are binary images of size $64\\times64$ where $1$ indicate the position of the brain MRI scan where a tumour is detetced. We will discard the prediction where no tumours have been detected and the rest of the test data will procced to the U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, preds = predict(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the test loss and accuracy of the model. However, this is only an analysis of the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_batch_loss, test_batch_acc = model_testing(model, test_dataloader, criterion, device, model_name=f'./models/CA_{scan_type}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our predicted results from Convolutional Autoencoder, we can find the original file names for their respective scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the original file names\n",
    "folders = [folder for folder in os.listdir(os.path.join(master_path, 'BraTS2021_Training_Data')) if folder != '.DS_Store']\n",
    "\n",
    "def index_converter(index):\n",
    "    return index // 155, 1 + index % 155 # image_idx, layer_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location of the predicted tumours are saved. We will also keep a list of scans where tumours are detected and another list of the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(master_path, 'pipeline_prediction', f'CA_{scan_type}_Area'), exist_ok=True)\n",
    "\n",
    "pred_no_tumour = []\n",
    "pred_tumour = []\n",
    "for i in range(len(test_indices)):\n",
    "    image_idx, layer_idx = index_converter(test_indices[i])\n",
    "    image_idx = folders[image_idx]\n",
    "    pred_label = preds[i]\n",
    "\n",
    "    rows_indices = torch.where(pred_label)[0]\n",
    "    cols_indices = torch.where(pred_label)[1]\n",
    "\n",
    "    if len(rows_indices) == 0 or len(cols_indices) == 0:\n",
    "        pred_no_tumour.append([str(image_idx).zfill(5), str(layer_idx)])\n",
    "        continue\n",
    "    \n",
    "    pred_tumour.append([str(image_idx).zfill(5), str(layer_idx)])\n",
    "    top_row = torch.min(rows_indices)\n",
    "    bottom_row = torch.max(rows_indices)\n",
    "    left_col = torch.min(cols_indices)\n",
    "    right_col = torch.max(cols_indices)\n",
    "\n",
    "    width = right_col - left_col + 1\n",
    "    height = bottom_row - top_row + 1\n",
    "\n",
    "    if width > height:\n",
    "        top_row = top_row - np.floor((width - height) / 2)\n",
    "        bottom_row = bottom_row + np.ceil((width - height) / 2)\n",
    "        if top_row < 0:\n",
    "            bottom_row = bottom_row - top_row\n",
    "            top_row = 0\n",
    "        elif bottom_row > 63:\n",
    "            top_row = top_row - (bottom_row - 63)\n",
    "            bottom_row = 63\n",
    "    elif height > width:\n",
    "        left_col = left_col - np.floor((height - width) / 2)\n",
    "        right_col = right_col + np.ceil((height - width) / 2)\n",
    "        if left_col < 0:\n",
    "            right_col = right_col - left_col\n",
    "            left_col = 0\n",
    "        elif right_col > 63:\n",
    "            left_col = left_col - (right_col - 63)\n",
    "            right_col = 63\n",
    "\n",
    "    path = os.path.join(master_path, 'pipeline_prediction', f'CA_{scan_type}_Predicted_Area', str(image_idx).zfill(5) + '_ROI_pred_' + str(layer_idx))\n",
    "    np.save(path, np.array([top_row, bottom_row, left_col, right_col]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tumour = np.array(pred_no_tumour)\n",
    "pred_no_tumour = np.array(pred_tumour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to 'preprocess' the MRI scans by dropping them into the dimension/location predicted in the first model. These will be used as the input to our U-Net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(master_path, 'pipeline_prediction', 'UNet_Test_Input'), exist_ok=True)\n",
    "\n",
    "pool = multiprocessing.Pool(processes=multiprocessing.cpu_count())  # Use all available CPU cores\n",
    "pool.map(convert_Unet_pred, pred_tumour)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_files = np.sort([image for image in os.listdir(os.path.join(master_path, 'pipeline_prediction', 'UNet_Test_Input')) if image != '.DS_Store'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our dataloader from the preprocessed images generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, image_path = os.path.join(master_path, 'pipeline_prediction', 'UNet_Test_Input'), transform = None):\n",
    "        'Initialisation'\n",
    "        self.image_path = image_path\n",
    "        self.image_names = np.sort([image for image in os.listdir(self.image_path) if image != '.DS_Store'])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Select sample\n",
    "        image_name = self.image_names[index]\n",
    "        \n",
    "        path_img = os.path.join(self.image_path, image_name)\n",
    "        image = np.load(path_img).astype(np.float32)\n",
    "        label = image\n",
    "        \n",
    "        if self.transform:\n",
    "            image, label = self.transform([image, label])\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "\n",
    "dataset = BraTSDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and load the trained model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Input is 32 x 32 x 1\n",
    "        ## Output is 32 x 32 x 4\n",
    "        \n",
    "        # Encoder\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3 32 x 32 x 1\n",
    "        self.e11 = nn.Conv2d(1, 64, kernel_size=3, padding=1) # output: 30x30x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, 64, 64)\n",
    "        # Encoder\n",
    "        xe11 = F.relu(self.e11(x))\n",
    "        xe12 = F.relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "\n",
    "        xe21 = F.relu(self.e21(xp1))\n",
    "        xe22 = F.relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "\n",
    "        xe31 = F.relu(self.e31(xp2))\n",
    "        xe32 = F.relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "\n",
    "        xe41 = F.relu(self.e41(xp3))\n",
    "        xe42 = F.relu(self.e42(xe41))\n",
    "        xp4 = self.pool4(xe42)\n",
    "\n",
    "        xe51 = F.relu(self.e51(xp4))\n",
    "        xe52 = F.relu(self.e52(xe51))\n",
    "        \n",
    "        # Decoder\n",
    "        xu1 = self.upconv1(xe52)\n",
    "        xu11 = torch.cat([xu1, xe42], dim=1)\n",
    "        xd11 = F.relu(self.d11(xu11))\n",
    "        xd12 = F.relu(self.d12(xd11))\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu22 = torch.cat([xu2, xe32], dim=1)\n",
    "        xd21 = F.relu(self.d21(xu22))\n",
    "        xd22 = F.relu(self.d22(xd21))\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu33 = torch.cat([xu3, xe22], dim=1)\n",
    "        xd31 = F.relu(self.d31(xu33))\n",
    "        xd32 = F.relu(self.d32(xd31))\n",
    "\n",
    "        xu4 = self.upconv4(xd32)\n",
    "        xu44 = torch.cat([xu4, xe12], dim=1)\n",
    "        xd41 = F.relu(self.d41(xu44))\n",
    "        xd42 = F.relu(self.d42(xd41))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd42)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Loss\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "model.load_state_dict(torch.load(f'./models/Unet_{scan_type}.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction now for U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, preds_UNet = predict(model, dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pipeline analysis. We have to consider the cases where some test images were not fed into the U-Net model. Firstly, we will extract the list of all scans, the lists of scans which we predicted and did not predict, and lists of scans which we should have predicted and not based on true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id(name):\n",
    "    name = os.path.splitext(name)[0]\n",
    "    return name.split('_')[1], name.split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_full_test_images = [str(folders[index_converter(test_indices[i])[0]])+'_seg_'+str(index_converter(test_indices[i])[1])+'.npy' for i in range(len(test_indices))]\n",
    "\n",
    "list_actual_input = [image for image in os.listdir(os.path.join(master_path, 'BraTS2021_Training_Data_Slice_Cropped', 'test', 'seg')) if image != '.DS_Store']\n",
    "list_actual_non_input = list(set(list_full_test_images).difference(set(list_actual_input)))\n",
    "list_predict_input = [pred_tumour[i][0] + '_seg_' + pred_tumour[i][1] + '.npy' for i in range(len(pred_tumour))]\n",
    "list_predict_non_input = [pred_no_tumour[i][0] + '_seg_' + pred_no_tumour[i][1] + '.npy' for i in range(len(pred_no_tumour))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can obtain the list of scans which fall into different categories of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = list(set(list_predict_input).intersection(set(list_actual_input)))\n",
    "FP = list(set(list_predict_input).difference(set(list_actual_input)))\n",
    "FN = list(set(list_actual_input).difference(set(list_predict_input)))\n",
    "TN = list(set(list_predict_non_input).intersection(set(list_actual_non_input)))\n",
    "\n",
    "TP_count = len(TP)\n",
    "FP_count = len(FP)\n",
    "FN_count = len(FN)\n",
    "TN_count = len(TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate for the final DICE score as the test accuracy. We have 4 different cases for each scan.\n",
    "\n",
    "1. If `TP`, compute DICE between the true tumour segmentation against our prediction.\n",
    "2. If `FP`, compute DICE between a blank image against our prediction.\n",
    "3. If `FN`, compute DICE between the true tumour segmentation against a blank image.\n",
    "4. If `FN`, conclude DICE is 1 for the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_prediction(image):\n",
    "    image = torch.argmax(image, dim=0)\n",
    "    image = torch.where(image == 3, torch.tensor(4), image)\n",
    "    return image\n",
    "\n",
    "def dice2d(pred, target):\n",
    "    return (pred == target).sum() / np.prod(pred.shape)\n",
    "\n",
    "def dice2d_exclude_zero(pred, target):\n",
    "    if np.prod(pred.shape) == np.sum((pred == 0) & (target == 0)):\n",
    "        return 1\n",
    "    return ((pred == target) & ((pred != 0) | (target != 0))).sum() / (np.prod(pred.shape)-np.sum((pred == 0) & (target == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop below finds the DICE scores for each scan in `TP` and `FP` categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dices = []\n",
    "dices_exclude_zero = []\n",
    "\n",
    "# loop through each image of preds_UNet\n",
    "for i in range(len(preds_UNet)):\n",
    "    \n",
    "    scan_name = 'BraTS2021_' + extract_id(testing_files[i])[0]\n",
    "    scan_no = extract_id(testing_files[i])[1]\n",
    "    dim_name = scan_name + '_ROI_pred_' + scan_no + '.npy'\n",
    "    top_row, bottom_row, left_col, right_col = np.load(os.path.join(master_path, 'pipeline_prediction', f'CA_{scan_type}_Area', dim_name)).astype(np.int32)\n",
    "    org_size = bottom_row - top_row + 1\n",
    "\n",
    "    pred = preds_UNet[i]\n",
    "    pred = rebuild_prediction(pred)\n",
    "    pred_resize = cv2.resize(pred.numpy(), [org_size, org_size], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    pred = np.zeros((64, 64))\n",
    "    if right_col < left_col:\n",
    "        left_col, right_col = right_col, left_col\n",
    "    if bottom_row < top_row:\n",
    "        top_row, bottom_row = bottom_row, top_row\n",
    "    \n",
    "    if bottom_row - top_row == right_col - left_col:\n",
    "        pred[top_row:bottom_row+1, left_col:right_col+1] = pred_resize\n",
    "    elif bottom_row - top_row > right_col - left_col:\n",
    "        pred[top_row:bottom_row, left_col:right_col+1] = pred_resize\n",
    "    else:\n",
    "        pred[top_row:bottom_row+1, left_col:right_col] = pred_resize\n",
    "    \n",
    "    seg_name = scan_name + '_seg_' + scan_no + '.npy'\n",
    "    seg = np.load(os.path.join(master_path, 'BraTS2021_Training_Data_Slice', scan_name, 'seg', seg_name))\n",
    "    \n",
    "    dices.append(dice2d(pred, seg))\n",
    "    dices_exclude_zero.append(dice2d_exclude_zero(pred, seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for `FN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through list of not predicted but should have\n",
    "for image in FN:\n",
    "    img_path = os.path.join(master_path, 'BraTS2021_Training_Data_Slice', 'BraTS2021_' + image.split('_')[1], 'seg', image)\n",
    "    img = np.load(img_path)\n",
    "    dice = np.count_nonzero(img == 0)/np.prod(img.shape)\n",
    "    dices.append(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICE_final = np.mean(dices) * (TP_count + FP_count + FN_count) / len(list_full_test_images) + 1 * TN_count / len(list_full_test_images)\n",
    "\n",
    "print(f'DICE Score: {DICE_final:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Images Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./prediction/', exist_ok=True)\n",
    "\n",
    "def visual_compare(id):\n",
    "    \n",
    "    _, axarr = plt.subplots(1,2)\n",
    "\n",
    "    # load original image and crop\n",
    "    org_scan_name = 'BraTS2021_' + extract_id(testing_files[id])[0]\n",
    "    org_scan = nib.load(os.path.join('./BraTS', 'BraTS2021_Training_Data', org_scan_name, org_scan_name + '_flair.nii.gz')).get_fdata()\n",
    "    org_scan = org_scan[:,:,int(extract_id(testing_files[id])[1])]\n",
    "\n",
    "    true = np.load(f'./BraTS/BraTS2021_Training_Data_Slice/BraTS2021_{extract_id(testing_files[id])[0]}/seg/BraTS2021_{extract_id(testing_files[id])[0]}_seg_{extract_id(testing_files[id])[1]}.npy')\n",
    "    \n",
    "    # De One Hot\n",
    "    pred = torch.argmax(preds_UNet[id], dim=0)\n",
    "    # Re-label class '3' to '4'\n",
    "    pred = torch.where(pred == 3, torch.tensor(4), pred)\n",
    "    top_row, bottom_row, left_col, right_col = np.load(f'./BraTS/CA_Flair_Area/BraTS2021_{extract_id(testing_files[id])[0]}_ROI_pred_{extract_id(testing_files[id])[1]}.npy')\n",
    "    # resize pred\n",
    "    org_size = bottom_row - top_row + 1\n",
    "    pred_resize = cv2.resize(pred.numpy(), [org_size, org_size], interpolation=cv2.INTER_NEAREST)\n",
    "    pred = np.zeros((64, 64))\n",
    "    if top_row > 0:\n",
    "        top_row, bottom_row = top_row - 1, bottom_row - 1\n",
    "    if left_col > 0:\n",
    "        left_col, right_col = left_col - 1, right_col - 1\n",
    "    if bottom_row - top_row == right_col - left_col:\n",
    "        pred[top_row:bottom_row+1, left_col:right_col+1] = pred_resize\n",
    "    elif bottom_row - top_row > right_col - left_col:\n",
    "        pred[top_row:bottom_row, left_col:right_col+1] = pred_resize\n",
    "    else:\n",
    "        pred[top_row:bottom_row+1, left_col:right_col] = pred_resize\n",
    "\n",
    "    axarr[0].imshow(CropAndResize(org_scan), cmap='gray')\n",
    "    masked_true = np.ma.masked_where(true == 0, true)\n",
    "    axarr[0].imshow(masked_true, alpha=1)\n",
    "    axarr[0].axis('off')\n",
    "\n",
    "    axarr[1].imshow(CropAndResize(org_scan), cmap='gray')\n",
    "    masked_pred = np.ma.masked_where(pred == 0, pred)\n",
    "    axarr[1].imshow(masked_pred, alpha=1)\n",
    "    axarr[1].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0)\n",
    "    plt.savefig(f'./prediction/BraTS2021_{extract_id(testing_files[id])[0]}_pred_{extract_id(testing_files[id])[1]}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in range(len(testing_files)):\n",
    "    visual_compare(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# os.path.join(master_path, 'BraTS2021_Training_Data_Slice')\n",
    "# os.path.join(master_path, 'BraTS2021_Training_Data_Slice_Cropped')\n",
    "# os.path.join(master_path, 'pipeline_prediction', f'CA_{scan_type}_Area')\n",
    "# os.path.join(master_path, 'pipeline_prediction', 'UNet_Test_Input')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
